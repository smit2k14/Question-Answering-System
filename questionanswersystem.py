# -*- coding: utf-8 -*-
"""QuestionAnswerSystem

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DaTitd07T7fHoHJfwXyz3AsLLXVC0kFe
"""

import numpy as np
import pandas as pd
import tensorflow as tf
import json
from keras_preprocessing import sequence
from keras.models import Sequential, Model
from keras.layers import LSTM, Embedding, Input, Bidirectional
from keras.layers import Dense, Flatten, Dropout, Lambda
from keras.layers import TimeDistributed
from keras.layers import RepeatVector
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.corpus import stopwords
import nltk
from nltk import word_tokenize
import re

with open('.../train-v2.0.json') as f:
    data = json.load(f)
    f.close()

stop_words = set(stopwords.words('english'))

title = []
paragraphs = []
question = []
answers = []
id = []
is_impossible = []
context = []
for d in data['data']:
    title.append(d['title'])
    a = []
    b = []
    for p in d['paragraphs']:
        for q in p['qas']:
            try:
                a.append([q['answers'][0]['text']])
                b.append([q['question']])
                id.append([q['id']])
                is_impossible.append([q['is_impossible']])
            except:
                continue
        answers.append(a)
        question.append(b)
        context.append(p['context'])

def remove_stopwords(text):
    words = word_tokenize(text)
    output_text = []
    for w in words:
        if w not in stop_words:
            output_text.append(w)
    output_text = ' '.join(output_text).replace(' , ',',').replace(' .','.').replace(' !','!')
    output_text = output_text.replace(' ?','?').replace(' : ',': ').replace(' \'', '\'')   
    return output_text

lemmatizer = WordNetLemmatizer()
def context_preprocessing(data):
    output_data = []
    for d in data:
        d = d.lower()
        d = remove_stopwords(d)
        d = lemmatizer.lemmatize(d)
        d = d.replace(',', ' [COM] ')
        d = d.replace('.', ' [EOS] ')
        d = d.replace("'s", '')
        o = ' '.join(re.sub("[\(\)\"\"\']","",d).split())
        output_data.append(o) 
    return output_data

output_context = context_preprocessing(context)

print(output_context[0])

vocab_size = 83000
tokenizer = Tokenizer(num_words= vocab_size)
tokenizer.fit_on_texts(output_context)

context_sequences = tokenizer.texts_to_sequences(output_context)

m = 0
for seq in context_sequences:
    m = max(m, len(seq))

data = pad_sequences(context_sequences, maxlen=m)
X, y = data[:,:-1],data[:,-1]
y = to_categorical(y, num_classes=vocab_size)

def question_preprocessing(questions):
    output_question = []
    for cont in questions:
        cont_ques = []
        for q in cont:
            q = q[0]
            q = lemmatizer.lemmatize(q)
            d = q.lower()
            d = remove_stopwords(d)
            d = lemmatizer.lemmatize(d)
            d = d.replace(',', ' [COM] ')
            d = d.replace('.', ' [EOS] ')
            d = d.replace('?', ' [EOS] ')  
            d = d.replace("'s", '')
            c = ' '.join(re.sub("[\(\)\"\"\']","",d).split())
            cont_ques.append(c)
        output_question.append(cont_ques)
    return output_question

output_question = question_preprocessing(question)

question_sequences = tokenizer.texts_to_sequences(output_question)

question_sequences = pad_sequences(question_sequences, maxlen=m)

def create_model():
    inp = Input(shape = (m-1, ))
    embedding = Embedding(vocab_size, 100)(inp)
    lstm_layer = LSTM(100)(embedding)
    dense = Dense(vocab_size, activation = 'softmax')(lstm_layer)
    dropout = Dropout(0.25)(dense)
    model = Model(inputs = inp, outputs = dropout)
    print(model.summary())
    return model

model = create_model()
model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy'])

model.fit(X, y, batch_size = 128, epochs = 50)

embeddings = model.layers[1].get_weights()[0]

def create_model_qans():
    inp = Input(shape = (m-1, 100, ))
    lstm_layer = Bidirectional(LSTM(128))(inp)
    dense_1 = Dense(vocab_size, activation = 'softmax')(lstm_layer)
    dropout_1 = Dropout(0.25)(dense_1)
    model = Model(inputs = inp, output = dropout_1)
    return model

model_qans = create_model_qans()

question_sequences = []
for que in output_question:
    que_sequence = tokenizer.texts_to_sequences(que)
    question_sequences.append(que_sequence)

m = 0
for seq in question_sequences:
    for s in seq:
        m = max(m, len(s))
print(m)

padded_question_sequence = []
for seq in question_sequences:
    q_seq = []
    for s in seq:
        q_seq.append(pad_sequences(seq, maxlen=m))
    padded_question_sequence.append(q_seq)

print(len(padded_question_sequence))

